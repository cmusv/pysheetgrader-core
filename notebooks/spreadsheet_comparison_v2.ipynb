{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreadsheet comparison and grading\n",
    "\n",
    "This notebook holds the exploratory code for comparing submissions against the key. It will also do grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sheetnames: ['Sheet3', 'Sheet3_CheckOrder']\n",
      "Grading sheetnames: ['Sheet3']\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Loading the key workbook.\n",
    "# The compared data will come from Sheet3, cell B6.\n",
    "\n",
    "key_data = load_workbook(\"../sample_excel_files/type_1_key.xlsx\", data_only=True)\n",
    "key_formulas = load_workbook(\"../sample_excel_files/type_1_key.xlsx\", data_only=False)\n",
    "\n",
    "print(f\"All sheetnames: {key_data.sheetnames}\")\n",
    "\n",
    "# This way, we assume all necessary sheets will have the _CheckOrder pair.\n",
    "grading_sheetnames = [name for name in key_data.sheetnames if name.find(\"_CheckOrder\") == -1]\n",
    "print(f\"Grading sheetnames: {grading_sheetnames}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key formula and grade parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing methods\n",
    "\n",
    "\n",
    "def get_grading_sheetnames(key_wb):\n",
    "    \"\"\"\n",
    "    Returns grading sheetnames from the passed key workbook.\n",
    "    \"\"\"\n",
    "    return [name for name in key_wb.sheetnames if name.find(\"_CheckOrder\") == -1]\n",
    "\n",
    "def get_sheet_grading_sequence(key_wb):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of sheetnames as key. Each key will have an ordered list of tuples,\n",
    "    which contains the cell name in that sheet and the grading rubric. Both are in strings.\n",
    "    \"\"\"\n",
    "    grading_dict = {}\n",
    "    sheetnames = get_grading_sheetnames(key_wb)\n",
    "    \n",
    "    for name in sheetnames:\n",
    "        order_sheet = key_wb[name + \"_CheckOrder\"]\n",
    "        key_sheet = key_wb[name]\n",
    "        \n",
    "        # Assumptions of the order sheet \n",
    "        # 1. The scoring column is always on B. (min_col=2, max_col=2)\n",
    "        # 2. The scoring column always has a header (min_row=2)\n",
    "        # 3. The scoring column is always in order        \n",
    "        for column in order_sheet.iter_cols(min_col=2, max_col=2, min_row=2):\n",
    "            # Assuming this for-loop will only be executed for B column\n",
    "            # And the reference cells always have comment text for rubric.\n",
    "            grading_dict[name] = [(cell.value, key_sheet[cell.value].comment.text) for cell in column]\n",
    "\n",
    "    return grading_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key grading sequence for each sheet:\n",
      "Sheet3:\n",
      "[('B6', 'Rubric:\\n\\t10P-C\\n'), ('B7', 'Rubric:\\n\\t5P-C\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Test the methods above\n",
    "\n",
    "key_grading_sequence = get_sheet_grading_sequence(key_data)\n",
    "\n",
    "print(\"Key grading sequence for each sheet:\")\n",
    "\n",
    "for sheetname in key_grading_sequence:\n",
    "    print(sheetname + \":\")\n",
    "    print(key_grading_sequence[sheetname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method definition\n",
    "import re\n",
    "\n",
    "# TODO: for clarity, perhaps it's better to return a class with properties instead of tuple in the future.\n",
    "def parse_grading_rubric(rubric):\n",
    "    \"\"\"\n",
    "    Returns a tuple of (score, type) for the passed rubric String.\n",
    "    The score is a number, could be an Integer or Float.\n",
    "    The type is a letter, \"C\" for Constant evaluation and \"F\" for Formula evaluation.\n",
    "    \"\"\"\n",
    "    grade_search = re.search('\\t(.+?)P', rubric)\n",
    "    type_search = re.search('P-(.+?)', rubric)\n",
    "    \n",
    "    grade = grade_search.group(1) if grade_search else None\n",
    "    grading_type = type_search.group(1) if type_search else None\n",
    "    \n",
    "    return grade, grading_type\n",
    "\n",
    "def parse_grading_criteria(criteria):\n",
    "    \"\"\"\n",
    "    Returns a tuple of (rubric, unit_tests) for passed criteria String.\n",
    "\n",
    "    The rubric is a tuple of (score, type), where the score is a number and type is a letter.\n",
    "        The letter \"C\" represents Constant evaluation and \"F\" represents Formula evaluation.\n",
    "        \n",
    "    The unit_tests is an array of String that can be parsed for unit testing the submission cell.\n",
    "    \"\"\"\n",
    "    criteria_by_line = criteria.split(sep=\"\\n\")\n",
    "        \n",
    "    # Assumption: \n",
    "    # 1. Grading rubric is always on the second line.\n",
    "    # 2. Unit tests is always on the fourth line forward.\n",
    "    rubric = criteria_by_line[1] if len(criteria_by_line) >= 2 else None\n",
    "    unit_tests = criteria_by_line[3:] if len(criteria_by_line) >= 4 else None\n",
    "\n",
    "    parsed_rubric = parse_grading_rubric(rubric) if rubric else None    \n",
    "    return parsed_rubric, unit_tests\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading rubric for cell B6: ('10', 'C')\n",
      "Unit tests for cell B6: None\n",
      "Grading rubric for cell B7: ('5', 'C')\n",
      "Unit tests for cell B7: None\n"
     ]
    }
   ],
   "source": [
    "# Testing above methods\n",
    "\n",
    "for sheetname in key_grading_sequence:\n",
    "    for cell_name, criteria in key_grading_sequence[sheetname]:\n",
    "        \n",
    "        parsed_rubric, unit_tests = parse_grading_criteria(criteria)        \n",
    "        print(f\"Grading rubric for cell {cell_name}: {parsed_rubric}\")\n",
    "        print(f\"Unit tests for cell {cell_name}: {unit_tests}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant value grading\n",
    "\n",
    "The cells below will be used to explore how to grade a submission against constant rubrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method definition\n",
    "\n",
    "# TODO: Use this method for the formula-based grading later.\n",
    "\n",
    "def grade_submission(submission_filepath, key_data_wb, key_formula_wb):\n",
    "    \"\"\"\n",
    "    Returns the grade for passed submission_filepath, based on key workbook - both data and formula version.\n",
    "    \"\"\"\n",
    "    submission_score = 0\n",
    "    \n",
    "    sub_data_wb = load_workbook(submission_filepath, data_only=True)\n",
    "    sub_formula_wb = load_workbook(submission_filepath, data_only=False)\n",
    "\n",
    "    # Assumption: Both the submitted file and key have similar sheetnames and cell structures.\n",
    "    grading_sheet_sequence = get_sheet_grading_sequence(key_data_wb)\n",
    "\n",
    "    for sheetname in grading_sheet_sequence:\n",
    "        \n",
    "        sub_data_sheet = sub_data_wb[sheetname]\n",
    "        sub_formula_sheet = sub_formula_wb[sheetname]\n",
    "        \n",
    "        key_data_sheet = key_data_wb[sheetname]\n",
    "        key_formula_sheet = key_formula_wb[sheetname]\n",
    "        \n",
    "        for cell_coord, criteria in grading_sheet_sequence[sheetname]:\n",
    "            \n",
    "            rubric, unit_tests = parse_grading_criteria(criteria)\n",
    "            score, grading_type = rubric\n",
    "            \n",
    "            if grading_type != \"C\":\n",
    "                pass\n",
    "            elif sub_data_sheet[cell_coord].value == key_data_sheet[cell_coord].value:\n",
    "                submission_score += int(score)\n",
    "                \n",
    "                \n",
    "    return submission_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade for ../sample_excel_files/type_1_key.xlsx:\t\t15\n",
      "Grade for ../sample_excel_files/type_1_sub_right_different_formula_1.xlsx:\t\t15\n",
      "Grade for ../sample_excel_files/type_1_sub_right_different_formula_2.xlsx:\t\t15\n",
      "Grade for ../sample_excel_files/type_1_sub_right_exact.xlsx:\t\t15\n",
      "Grade for ../sample_excel_files/type_1_sub_wrong_constant_value.xlsx:\t\t15\n",
      "Grade for ../sample_excel_files/type_1_sub_wrong_different_formula_and_result.xlsx:\t\t0\n",
      "Grade for ../sample_excel_files/type_1_sub_wrong_different_result.xlsx:\t\t0\n"
     ]
    }
   ],
   "source": [
    "# Test the method above\n",
    "\n",
    "submission_filepaths = [\n",
    "    \"../sample_excel_files/type_1_key.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_right_different_formula_1.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_right_different_formula_2.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_right_exact.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_wrong_constant_value.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_wrong_different_formula_and_result.xlsx\",\n",
    "    \"../sample_excel_files/type_1_sub_wrong_different_result.xlsx\"\n",
    "]\n",
    "\n",
    "for file in submission_filepaths:\n",
    "    score = grade_submission(file, key_data, key_formulas)\n",
    "    print(f\"Grade for {file}:\\n{score}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
